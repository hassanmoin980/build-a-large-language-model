{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d8c67c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ca36dfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0+cu130\n",
      "13.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check PyTorch version and CUDA availability\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993c3d86",
   "metadata": {},
   "source": [
    "# A.2 Understanding Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692c6cbe",
   "metadata": {},
   "source": [
    "## Scalars, vectores, matrices, and tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a45297d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-D tensor (scalar) from a Python integer: 1, having shape torch.Size([]), dimension = 0\n",
      "1-D tensor (vector) from a Python list: tensor([1, 2, 3]), having shape torch.Size([3]), dimension = 1\n",
      "2-D tensor from a nested Python list: tensor([[1, 2],\n",
      "        [3, 4]]), having shape torch.Size([2, 2]), dimension = 2\n",
      "3-D tensor from a nested Python list: tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]]), having shape torch.Size([2, 2, 2]), dimension = 3\n",
      "4-D tensor from a nested Python list: tensor([[[[1, 2, 9, 9],\n",
      "          [3, 4, 9, 9]],\n",
      "\n",
      "         [[5, 6, 9, 9],\n",
      "          [7, 8, 9, 9]],\n",
      "\n",
      "         [[5, 6, 9, 9],\n",
      "          [7, 8, 9, 9]]],\n",
      "\n",
      "\n",
      "        [[[1, 2, 9, 9],\n",
      "          [3, 4, 9, 9]],\n",
      "\n",
      "         [[5, 6, 9, 9],\n",
      "          [7, 8, 9, 9]],\n",
      "\n",
      "         [[5, 6, 9, 9],\n",
      "          [7, 8, 9, 9]]],\n",
      "\n",
      "\n",
      "        [[[1, 2, 9, 9],\n",
      "          [3, 4, 9, 9]],\n",
      "\n",
      "         [[5, 6, 9, 9],\n",
      "          [7, 8, 9, 9]],\n",
      "\n",
      "         [[5, 6, 9, 9],\n",
      "          [7, 8, 9, 9]]]]), having shape torch.Size([3, 3, 2, 4]), dimension = 4\n"
     ]
    }
   ],
   "source": [
    "# To create a 0-D, 1-D, 2-D, and 3-D tensor in PyTorch:\n",
    "tensor0d = torch.tensor(1)\n",
    "tensor1d = torch.tensor([1, 2, 3])\n",
    "tensor2d = torch.tensor([[1, 2], [3, 4]])\n",
    "tensor3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "tensor4d = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [[1, 2, 9, 9], [3, 4, 9, 9]],\n",
    "            [[5, 6, 9, 9], [7, 8, 9, 9]],\n",
    "            [[5, 6, 9, 9], [7, 8, 9, 9]],\n",
    "        ],\n",
    "        [\n",
    "            [[1, 2, 9, 9], [3, 4, 9, 9]],\n",
    "            [[5, 6, 9, 9], [7, 8, 9, 9]],\n",
    "            [[5, 6, 9, 9], [7, 8, 9, 9]],\n",
    "        ],\n",
    "        [\n",
    "            [[1, 2, 9, 9], [3, 4, 9, 9]],\n",
    "            [[5, 6, 9, 9], [7, 8, 9, 9]],\n",
    "            [[5, 6, 9, 9], [7, 8, 9, 9]],\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"0-D tensor (scalar) from a Python integer: {tensor0d}, having shape {tensor0d.size()}, dimension = {tensor0d.dim()}\"\n",
    ")\n",
    "print(\n",
    "    f\"1-D tensor (vector) from a Python list: {tensor1d}, having shape {tensor1d.size()}, dimension = {tensor1d.dim()}\"\n",
    ")\n",
    "print(\n",
    "    f\"2-D tensor from a nested Python list: {tensor2d}, having shape {tensor2d.size()}, dimension = {tensor2d.dim()}\"\n",
    ")\n",
    "print(\n",
    "    f\"3-D tensor from a nested Python list: {tensor3d}, having shape {tensor3d.size()}, dimension = {tensor3d.dim()}\"\n",
    ")\n",
    "print(\n",
    "    f\"4-D tensor from a nested Python list: {tensor4d}, having shape {tensor4d.size()}, dimension = {tensor4d.dim()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d6c704",
   "metadata": {},
   "source": [
    "## Tensor data types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169fb01b",
   "metadata": {},
   "source": [
    "> By default, python *integers* create tensors of dtype torch.int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c89ca713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(tensor0d.dtype)\n",
    "print(tensor1d.dtype)\n",
    "print(tensor2d.dtype)\n",
    "print(tensor3d.dtype)\n",
    "print(tensor4d.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fe2db8",
   "metadata": {},
   "source": [
    "> By default, python *floats* create tensors of dtype torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6f557554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "tensor1d_float = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(tensor1d_float.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3685b7",
   "metadata": {},
   "source": [
    "> **Note**: GPU architectures are optimized for 32-bit computations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d36b2fd",
   "metadata": {},
   "source": [
    "> To change a tensor's dtype, use the .to() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6ee0bd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "tensor1d_float64 = tensor1d_float.to(torch.float64)\n",
    "print(tensor1d_float64.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2659b13b",
   "metadata": {},
   "source": [
    "## Common PyTorch tensor operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7035c3",
   "metadata": {},
   "source": [
    "### Create new tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b6d1bc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor2d = torch.tensor([[1, 2], [3, 4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b68d941",
   "metadata": {},
   "source": [
    "### Reshape tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b8539222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(tensor2d)\n",
    "print(tensor2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "107d47b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor2d.reshape(4, 1))\n",
    "print(tensor2d.view(4, 1))  # Both are similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d5932ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 3],\n",
      "        [2, 4]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor2d.T)  # Transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcac094a",
   "metadata": {},
   "source": [
    "### Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "494c955c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5, 11],\n",
      "        [11, 25]])\n",
      "tensor([[ 5, 11],\n",
      "        [11, 25]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor2d.matmul(tensor2d.T))\n",
    "print(tensor2d @ tensor2d.T)  # Both are similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e39ae62",
   "metadata": {},
   "source": [
    "# A.3 Seeing models as computation graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1176eb6",
   "metadata": {},
   "source": [
    "### A logistic regression forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "88443a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = tensor([1.]), dtype = torch.float32\n",
      "x1 = tensor([1.1000]), dtype = torch.float32\n",
      "w1 = tensor([2.2000]), dtype = torch.float32\n",
      "b = tensor([0.]), dtype = torch.float32\n",
      "z = tensor([2.4200]), dtype = torch.float32\n",
      "a = tensor([0.9183]), dtype = torch.float32\n",
      "loss = 0.0851878821849823, dtype = torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F  # For activation functions\n",
    "\n",
    "y = torch.tensor([1.0])  # Target label\n",
    "print(f\"y = {y}, dtype = {y.dtype}\")\n",
    "\n",
    "x1 = torch.tensor([1.1])  # Input feature\n",
    "print(f\"x1 = {x1}, dtype = {x1.dtype}\")\n",
    "\n",
    "w1 = torch.tensor([2.2])  # Weight\n",
    "print(f\"w1 = {w1}, dtype = {w1.dtype}\")\n",
    "\n",
    "b = torch.tensor([0.0])  # Bias\n",
    "print(f\"b = {b}, dtype = {b.dtype}\")\n",
    "\n",
    "z = x1 * w1 + b  # Linear transformation (logits)\n",
    "print(f\"z = {z}, dtype = {z.dtype}\")\n",
    "\n",
    "a = torch.sigmoid(z)  # Sigmoid activation\n",
    "print(f\"a = {a}, dtype = {a.dtype}\")\n",
    "\n",
    "loss = F.binary_cross_entropy(a, y)  # Binary cross-entropy loss\n",
    "print(f\"loss = {loss}, dtype = {loss.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6241a178",
   "metadata": {},
   "source": [
    "# A.4 Automatic differentiation made easy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991a1e83",
   "metadata": {},
   "source": [
    "> **Note**: PyTorch will build a computation graph internally by default if one of its terminal nodes has the `requires_grad = True`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ab597a",
   "metadata": {},
   "source": [
    "### Computing gradients via autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "232e8598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = tensor([1.])\n",
      "x1 = tensor([1.1000])\n",
      "w1 = tensor([2.2000], requires_grad=True)\n",
      "b = tensor([0.], requires_grad=True)\n",
      "z = tensor([2.4200], grad_fn=<AddBackward0>)\n",
      "a = tensor([0.9183], grad_fn=<SigmoidBackward0>)\n",
      "loss = 0.0851878821849823\n",
      "Gradient of loss w.r.t w1: (tensor([-0.0898]),)\n",
      "Gradient of loss w.r.t b: (tensor([-0.0817]),)\n",
      "Gradient of loss w.r.t w1 using backward(): tensor([-0.0898])\n",
      "Gradient of loss w.r.t b using backward(): tensor([-0.0817])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.autograd import (\n",
    "    grad,\n",
    ")  # Manual grad function helpful for experimentation, debugging, and understanding autograd mechanics.\n",
    "\n",
    "y = torch.tensor([1.0])  # Target label\n",
    "print(f\"y = {y}\")\n",
    "\n",
    "x1 = torch.tensor([1.1])  # Input feature\n",
    "print(f\"x1 = {x1}\")\n",
    "\n",
    "w1 = torch.tensor([2.2], requires_grad=True)  # Weight\n",
    "print(f\"w1 = {w1}\")\n",
    "\n",
    "b = torch.tensor([0.0], requires_grad=True)  # Bias\n",
    "print(f\"b = {b}\")\n",
    "\n",
    "z = x1 * w1 + b  # Linear transformation (logits)\n",
    "print(f\"z = {z}\")\n",
    "\n",
    "a = torch.sigmoid(z)  # Sigmoid activation\n",
    "print(f\"a = {a}\")\n",
    "\n",
    "loss = F.binary_cross_entropy(a, y)  # Binary cross-entropy loss\n",
    "print(f\"loss = {loss}\")\n",
    "\n",
    "grad_L_w1 = grad(loss, w1, retain_graph=True)  # Compute gradient of loss w.r.t w1\n",
    "print(f\"Gradient of loss w.r.t w1: {grad_L_w1}\")\n",
    "\n",
    "grad_L_b = grad(loss, b, retain_graph=True)  # Compute gradient of loss w.r.t b\n",
    "print(f\"Gradient of loss w.r.t b: {grad_L_b}\")\n",
    "\n",
    "loss.backward()  # Backpropagation to compute gradients\n",
    "print(f\"Gradient of loss w.r.t w1 using backward(): {w1.grad}\")\n",
    "print(f\"Gradient of loss w.r.t b using backward(): {b.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef15999d",
   "metadata": {},
   "source": [
    "# A.5 Implementing multilayer neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a247067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super().__init__()  # Initialize the base class\n",
    "        self.hidden_layer_1 = torch.nn.Linear(num_inputs, 30)\n",
    "        self.hidden_layer_2 = torch.nn.Linear(30, 20)\n",
    "        self.output_layer = torch.nn.Linear(20, num_outputs)\n",
    "\n",
    "        self.network = torch.nn.Sequential(\n",
    "            self.hidden_layer_1,\n",
    "            torch.nn.ReLU(),\n",
    "            self.hidden_layer_2,\n",
    "            torch.nn.ReLU(),\n",
    "            self.output_layer,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.network(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = NeuralNetwork(50, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0285906e",
   "metadata": {},
   "source": [
    "> After instantiating `self.layers = Sequential(...)` in the `__init__` constructor, we just have to call the `self.layers` instead of calling each layer individually in the `NeuralNetwork`'s `forward`\n",
    "method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a3c68054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (hidden_layer_1): Linear(in_features=50, out_features=30, bias=True)\n",
      "  (hidden_layer_2): Linear(in_features=30, out_features=20, bias=True)\n",
      "  (output_layer): Linear(in_features=20, out_features=3, bias=True)\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e69908ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters in the model: 2213\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total number of trainable parameters in the model: {num_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf609a5d",
   "metadata": {},
   "source": [
    "> Note that each parameter for which `requires_grad=True` counts as a trainable parameter and will be updated during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c003f782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0577,  0.0047, -0.0702,  ...,  0.0222,  0.1260,  0.0865],\n",
      "        [ 0.0502,  0.0307,  0.0333,  ...,  0.0951,  0.1134, -0.0297],\n",
      "        [ 0.1077, -0.1108,  0.0122,  ...,  0.0108, -0.1049, -0.1063],\n",
      "        ...,\n",
      "        [-0.0787,  0.1259,  0.0803,  ...,  0.1218,  0.1303, -0.1351],\n",
      "        [ 0.1359,  0.0175, -0.0673,  ...,  0.0674,  0.0676,  0.1058],\n",
      "        [ 0.0790,  0.1343, -0.0293,  ...,  0.0344, -0.0971, -0.0509]],\n",
      "       requires_grad=True)\n",
      "torch.Size([30, 50])\n"
     ]
    }
   ],
   "source": [
    "print(model.network[0].weight)\n",
    "print(model.network[0].weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c62f20dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-0.1250,  0.0513,  0.0366,  0.0075,  0.0509,  0.0545, -0.0393,  0.0924,\n",
      "        -0.1412, -0.1232, -0.1063,  0.0081, -0.1249,  0.0101, -0.0019, -0.1298,\n",
      "         0.1388, -0.0330,  0.1017,  0.1247, -0.0554, -0.0417,  0.1388,  0.0159,\n",
      "         0.1215,  0.0385,  0.0769, -0.1224, -0.0279,  0.0991],\n",
      "       requires_grad=True)\n",
      "torch.Size([30])\n"
     ]
    }
   ],
   "source": [
    "print(model.network[0].bias)\n",
    "print(model.network[0].bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cb3d8e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2961, 0.5166, 0.2517, 0.6886, 0.0740, 0.8665, 0.1366, 0.1025, 0.1841,\n",
      "         0.7264, 0.3153, 0.6871, 0.0756, 0.1966, 0.3164, 0.4017, 0.1186, 0.8274,\n",
      "         0.3821, 0.6605, 0.8536, 0.5932, 0.6367, 0.9826, 0.2745, 0.6584, 0.2775,\n",
      "         0.8573, 0.8993, 0.0390, 0.9268, 0.7388, 0.7179, 0.7058, 0.9156, 0.4340,\n",
      "         0.0772, 0.3565, 0.1479, 0.5331, 0.4066, 0.2318, 0.4545, 0.9737, 0.4606,\n",
      "         0.5159, 0.4220, 0.5786, 0.9455, 0.8057]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "X = torch.rand((1, 50))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7058ca38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1262,  0.1080, -0.1792]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = model(X)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f11c742",
   "metadata": {},
   "source": [
    "> when we use a model for inference (for instance, making predictions) rather than training, it is a best practice to use the `torch.no_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "65e2180d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1262,  0.1080, -0.1792]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(X)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7cc4a785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3113, 0.3934, 0.2952]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = torch.softmax(model(X), dim=1)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9fa181",
   "metadata": {},
   "source": [
    "# A.6 Setting up efficient data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd6cafd",
   "metadata": {},
   "source": [
    "### Custom Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3290a94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(\n",
    "    [\n",
    "        [-1.2, 3.1],\n",
    "        [-0.9, 2.9],\n",
    "        [-0.5, 2.6],\n",
    "        [2.3, -1.1],\n",
    "        [2.7, -1.5],\n",
    "    ]\n",
    ")\n",
    "\n",
    "y_train = torch.tensor([0, 0, 0, 1, 1])\n",
    "\n",
    "X_test = torch.tensor(\n",
    "    [\n",
    "        [-0.8, 2.8],\n",
    "        [2.6, -1.6],\n",
    "    ]\n",
    ")\n",
    "\n",
    "y_test = torch.tensor([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db17f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class ToyDataset(Dataset):  # 3 main components: __init__, __getitem__, __len__\n",
    "    def __init__(self, X, y):\n",
    "        self.features = X\n",
    "        self.labels = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        one_x = self.features[index]\n",
    "        one_y = self.labels[index]\n",
    "        return one_x, one_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "\n",
    "train_ds = ToyDataset(X_train, y_train)\n",
    "test_ds = ToyDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bd34f40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in training dataset: 5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of samples in training dataset: {len(train_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c89bdc",
   "metadata": {},
   "source": [
    "### Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "69a1e42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Create DataLoader objects for batching and shuffling\n",
    "train_loader = DataLoader(dataset=train_ds, batch_size=2, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(dataset=test_ds, batch_size=2, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b672db29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "Features:\n",
      "tensor([[-0.5000,  2.6000],\n",
      "        [ 2.7000, -1.5000]])\n",
      "Labels:\n",
      "tensor([0, 1])\n",
      "\n",
      "Batch 2:\n",
      "Features:\n",
      "tensor([[-0.9000,  2.9000],\n",
      "        [ 2.3000, -1.1000]])\n",
      "Labels:\n",
      "tensor([0, 1])\n",
      "\n",
      "Batch 3:\n",
      "Features:\n",
      "tensor([[-1.2000,  3.1000]])\n",
      "Labels:\n",
      "tensor([0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the training DataLoader and print batches\n",
    "for idx, (batch_x, batch_y) in enumerate(train_loader):\n",
    "    print(f\"Batch {idx + 1}:\")\n",
    "    print(f\"Features:\\n{batch_x}\")\n",
    "    print(f\"Labels:\\n{batch_y}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bde8e52",
   "metadata": {},
   "source": [
    "> It is recommended to set `drop_last=True` in the DataLoader during training to ensure that all batches are of equal size, which can help maintain consistency during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d8497089",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=train_ds, batch_size=2, shuffle=True, num_workers=0, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d03259d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "Features:\n",
      "tensor([[ 2.3000, -1.1000],\n",
      "        [-1.2000,  3.1000]])\n",
      "Labels:\n",
      "tensor([1, 0])\n",
      "\n",
      "Batch 2:\n",
      "Features:\n",
      "tensor([[-0.5000,  2.6000],\n",
      "        [ 2.7000, -1.5000]])\n",
      "Labels:\n",
      "tensor([0, 1])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the training DataLoader and print batches\n",
    "for idx, (batch_x, batch_y) in enumerate(train_loader):\n",
    "    print(f\"Batch {idx + 1}:\")\n",
    "    print(f\"Features:\\n{batch_x}\")\n",
    "    print(f\"Labels:\\n{batch_y}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a57917c",
   "metadata": {},
   "source": [
    "# A.7 A typical training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "20c8992e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/003 | Batch: 001/002 | Train Loss: 0.75\n",
      "Epoch: 001/003 | Batch: 002/002 | Train Loss: 0.65\n",
      "Epoch: 002/003 | Batch: 001/002 | Train Loss: 0.44\n",
      "Epoch: 002/003 | Batch: 002/002 | Train Loss: 0.13\n",
      "Epoch: 003/003 | Batch: 001/002 | Train Loss: 0.03\n",
      "Epoch: 003/003 | Batch: 002/002 | Train Loss: 0.00\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "model = NeuralNetwork(num_inputs=2, num_outputs=2)  # Re-initialize the model\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=0.5\n",
    ")  # Tell the optimizer which parameters to optimize\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()  # Clear previous gradients to prevent unintended gradient accumulation\n",
    "        logits = model(features)  # Forward pass\n",
    "        loss = F.cross_entropy(logits, labels)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update model parameters (previous weights)\n",
    "\n",
    "        # Logging\n",
    "        print(\n",
    "            f\"Epoch: {epoch + 1:03d}/{num_epochs:03d}\"\n",
    "            f\" | Batch: {batch_idx + 1:03d}/{len(train_loader):03d}\"\n",
    "            f\" | Train Loss: {loss:.2f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9292d440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "752\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "648da7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.8569, -4.1618],\n",
      "        [ 2.5382, -3.7548],\n",
      "        [ 2.0944, -3.1820],\n",
      "        [-1.4814,  1.4816],\n",
      "        [-1.7176,  1.7342]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "    outputs = model(X_train)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d976d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9991, 0.0009],\n",
      "        [0.9982, 0.0018],\n",
      "        [0.9949, 0.0051],\n",
      "        [0.0491, 0.9509],\n",
      "        [0.0307, 0.9693]])\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(\n",
    "    sci_mode=False\n",
    ")  # Disable scientific notation for better readability\n",
    "probs = torch.softmax(\n",
    "    outputs, dim=1\n",
    ")  # Convert logits to probabilities where dim means axis along which softmax is computed\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1d96f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "predicted_classes = torch.argmax(\n",
    "    probs, dim=1\n",
    ")  # Get the class with the highest probability per row\n",
    "print(predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e5a7780e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True])\n",
      "tensor(5)\n"
     ]
    }
   ],
   "source": [
    "print(predicted_classes == y_train)\n",
    "print(torch.sum(predicted_classes == y_train))  # Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f912bad",
   "metadata": {},
   "source": [
    "> Function to compute the prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bf9e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, dataloader):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "build-a-large-language-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
